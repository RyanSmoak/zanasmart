# -*- coding: utf-8 -*-
"""exam_sem_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xVr3iwCnkaqqJpZha8fSsXYDsEkoU601
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain openai sentence_transformers

import os
from langchain_text_splitters import CharacterTextSplitter
from sentence_transformers import SentenceTransformer
import torch.nn.functional as F
from typing import Dict

os.environ["OPENAI_API_KEY"] = ""
import warnings
# Ignore all warnings
warnings.filterwarnings("ignore")


#a function to split our answers on an appearance of new line character
def answer_splitter(answers: str)-> list[dict]:
  #create an instance of the splitter
  text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 1,
    chunk_overlap = 0,
    length_function = len
  )

  answers_split = text_splitter.split_text(answers)

  answer_stat = []
  for i in range(len(answers_split)):
    answer_stat.append(
        {
            "answer_line_len": len(answers_split[i]),
            "answer_line_text": answers_split[i],
            "answer_embedding": None
        }
    )

  return answer_stat

#create embeddings for each answer
def embed(chunks: list[dict])->list[dict]:
  embedding_model = SentenceTransformer(
    model_name_or_path = "all-mpnet-base-v2",
    device = "cpu")

  sentences = []

  for i in range(len(chunks)):
    sentences.append(chunks[i].get("answer_line_text"))

  embeddings = embedding_model.encode(sentences, convert_to_tensor=True)
  embeddings_dict = dict(zip(sentences, embeddings))

  for i, embedding in enumerate(embeddings):
    chunks[i].update({"answer_embedding":embedding})

  return chunks


#cosine similarity function
def cos_sim(tens1, tens2):
  return F.cosine_similarity(tens1, tens2, dim=1)


#functionize our semantic search to be used for questions with 'points' type answers
def sem_search_points(
    scheme_chunks: list[dict],
    student_chunks: list[dict])-> int:

  correct_answers = 0

  #extract the embeddings from the chunks
  scheme_embeddings = [scheme_chunks[i].get("answer_embedding") for i in range(len(scheme_chunks))]
  student_embeddings = [student_chunks[i].get("answer_embedding") for i in range(len(student_chunks))]

  similarities = {}

  for i in range(len(student_embeddings)):
    for j in range(len(scheme_embeddings)):
      key = f"student_answer{i+1}_vs_scheme_answer{j+1}"
      similarities[key]=cos_sim(student_embeddings[i].reshape(1, -1), scheme_embeddings[j])

  for pair, sim in similarities.items():
    if (sim>0.8):
      print(f"{pair}:{sim}")
      correct_answers += 1

  return correct_answers

#functionize our semantic search to be used for questions with 'points' type answers
def analyse_answers(answers_scheme: str, answers_student:str):
  #marks=  sem_search_points(embed(answer_splitter(answers_scheme)), embed(answer_splitter(answers_student)))
  result = {
      "marks": 0
  }
  return result